{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In search of the stars...\n",
    "\n",
    "Let's try to reproduce an iconic Van Gogh style transfer from the 2016 Gatys et al paper. See [Readme](REAME.md) for details.\n",
    "\n",
    "In this notebook, code from [Style Transfer Excersize notebook](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/style-transfer/Style_Transfer_Exercise.ipynb) of [Udacity/deep-learning-v2-pytorch](https://github.com/udacity/deep-learning-v2-pytorch) is heavily borrowed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image manipulation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    ''' Load in and transform an image, making sure the image\n",
    "       is <= 400 pixels in the x-y dims.'''\n",
    "    \n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # large images will slow down processing\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "    \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "        \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for un-normalizing an image \n",
    "# and converting it from a Tensor image to a NumPy image for display\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Tranfer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping of layer names to the names found in the paper for the content representation and the style representation.\n",
    "def get_features(image, model, layers=None):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## mapping layer names of PyTorch's VGGNet to names from the paper\n",
    "    ## Need the layers for the content and style representations of an image\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1', '2': 'conv1_2', \n",
    "                  '5': 'conv2_1', '7': 'conv2_2',\n",
    "                  '10': 'conv3_1', '12': 'conv3_2', '14': 'conv3_3', '16': 'conv3_4', \n",
    "                  '19': 'conv4_1', '21': 'conv4_2', '23': 'conv4_3', '25': 'conv4_4', \n",
    "                  '28': 'conv5_1', '30': 'conv5_2', '32': 'conv5_3', '34': 'conv5_4'}\n",
    "\n",
    "    features = {}\n",
    "    x = image\n",
    "    \n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Gram matrix fuction\n",
    "def gram_matrix(tensor):\n",
    "    \"\"\" Calculate the Gram Matrix of a given tensor \n",
    "        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ## get the batch_size, depth, height, and width of the Tensor\n",
    "    batch_size, d, h, w = tensor.size()\n",
    "    ## reshape it, so we're multiplying the features for each channel\n",
    "    new_t = tensor.view(d, h * w)\n",
    "    ## calculate the gram matrix\n",
    "    gram = torch.mm(new_t, new_t.t())\n",
    "    \n",
    "    return gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to initialize target image\n",
    "def initialize_target(source, random=None):\n",
    "    \"\"\"Make a new image for future target\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    source = either content or style, it will be cloned\n",
    "    \n",
    "    random = initialize a random image, in this case dimentions are taken from source, \n",
    "             and degree of randomness can also be specified.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Initialized target image.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if random is not None:\n",
    "        target = torch.rand_like(source) * random + source * (1 - random)\n",
    "        \n",
    "    else:\n",
    "        target = source.clone()\n",
    "\n",
    "    return target.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define content, style and total losses\n",
    "def style_transfer_loss(target_features, content_features, style_grams, \n",
    "                        style_weights, content_weight, style_weight, \n",
    "                        return_all_losses=False):\n",
    "    \n",
    "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "    \n",
    "    # the style loss\n",
    "    # initialize the style loss to 0\n",
    "    style_loss = 0\n",
    "    \n",
    "    # iterate through each style layer and add to the style loss\n",
    "    for layer in style_weights:\n",
    "        # get the \"target\" style representation for the layer\n",
    "        target_feature = target_features[layer]\n",
    "        _, d, h, w = target_feature.shape\n",
    "        \n",
    "        ## Calculate the target gram matrix\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        \n",
    "        ## get the \"style\" style representation\n",
    "        style_gram = style_grams[layer]\n",
    "        \n",
    "        ## Calculate the style loss for one layer, weighted appropriately\n",
    "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "        \n",
    "        # add to the style loss\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "        \n",
    "        \n",
    "    ## calculate the *total* loss\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    \n",
    "    return total_loss if not return_all_losses else (total_loss, content_loss, style_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper to report loss during iterations\n",
    "def report_loss(target_features, content_features, style_grams, \n",
    "                style_weights, content_weight, style_weight):\n",
    "    losses_desc = [\"total\", \"content\", \"style\"]\n",
    "    losses = style_transfer_loss(target_features, content_features, style_grams, \n",
    "                                 style_weights, content_weight, style_weight, \n",
    "                                return_all_losses=True)\n",
    "    losses_map = map(torch.Tensor.item, losses)\n",
    "    loss_dict = {a : \"{0:.2f}\".format(b) for a,b in zip(losses_desc, losses_map)}\n",
    "    loss_report = 'Loss: {}'.format(loss_dict)\n",
    "    return loss_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a metric of similarity between a generated image \n",
    "# and the desired reference image (true target)\n",
    "# This is MAE\n",
    "def true_target_loss(true_target, target):\n",
    "    return torch.mean(torch.abs(true_target-target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to generate style_weights for grid search\n",
    "# adapted from https://stackoverflow.com/questions/51908760/rewriting-a-function-that-creates-combinations-of-numbers-with-a-fixed-sum-as-a\n",
    "# it generates lists with length_of_list integer values, with a condition that the sum is fixed_sum.\n",
    "# for our case of style_weights for 5 layers, we want length_of_list=5, fixed_sum=10 9and then divide by 10)\n",
    "\n",
    "def combinations_fixed_sum(fixed_sum, length_of_list, lst=[]):\n",
    "    if length_of_list == 1:\n",
    "        lst += [fixed_sum]\n",
    "        yield lst\n",
    "    else:\n",
    "        for i in range(fixed_sum+1):\n",
    "            yield from combinations_fixed_sum(i, length_of_list-1, lst + [fixed_sum-i])\n",
    "\n",
    "# exclude elements with 0 weights and dived\n",
    "#print(list(map(lambda x: [y/10 for y in x], filter(lambda x: not 0 in x, combinations_fixed_sum(10, 5)))))\n",
    "#[[0.6, 0.1, 0.1, 0.1, 0.1], [0.5, 0.2, 0.1, 0.1, 0.1], [0.5, 0.1, 0.2, 0.1, 0.1], ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemetaton of style transfer method in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a complete image generation routine to pass to the Experiment run.\n",
    "\n",
    "# each Tune trial is executed with current dir changed to trials's dir  \n",
    "# in ~/ray_results/my_experiment\n",
    "# But we need to get images from the repo dir, save it\n",
    "cwd = os.getcwd() + '/'\n",
    "\n",
    "def generate_image_with_config(config, reporter):\n",
    "    \n",
    "    #config \n",
    "    config_keys = [\n",
    "        'style_weights', \n",
    "        'content_weight', 'style_weight',\n",
    "        'steps' \n",
    "    ]\n",
    "    \n",
    "    (\n",
    "        style_weights_values, \n",
    "        content_weight, style_weight,\n",
    "        steps\n",
    "    ) = [config[x] for x in config_keys]\n",
    "    \n",
    "    show_every = None\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    style_weights_layers = ['conv1_1', 'conv2_1', 'conv3_1','conv4_1','conv5_1']\n",
    "    \n",
    "    style_weights = {x[0]:x[1] for x in zip(style_weights_layers, style_weights_values)}\n",
    "    \n",
    "    # get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
    "    vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "    # freeze all VGG parameters since we're only optimizing the target image\n",
    "    for param in vgg.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    \n",
    "    # move the model to GPU, if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    vgg.to(device)\n",
    "    \n",
    "    # load in content and style image\n",
    "    content_image = cwd + 'Tuebingen_Neckarfront_gatys_paper_002.jpg'\n",
    "    content = load_image(content_image).to(device)\n",
    "    \n",
    "    # Resize style to match content, makes code easier\n",
    "    style_image = cwd + 'Van_Gogh_Starry_Night_gatys_paper_004.jpg'\n",
    "    style = load_image(style_image, shape=content.shape[-2:]).to(device)\n",
    "\n",
    "    # load in true target image\n",
    "    # Resize  to match content, makes code easier\n",
    "    true_target_image = cwd + 'Van_Gogh_true_target_gatys_paper_021.jpg'\n",
    "    true_target = load_image(true_target_image, shape=content.shape[-2:]).to(device)\n",
    "    \n",
    "    #initialize target \n",
    "    target = initialize_target(content)\n",
    "   \n",
    "    \n",
    "    # get content and style features only once before forming the target image\n",
    "    content_features = get_features(content, vgg)\n",
    "    style_features = get_features(style, vgg)\n",
    "\n",
    "    # calculate the gram matrices for each layer of our style representation\n",
    "    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "    \n",
    "    \n",
    "    #optimizer = optim.LBFGS([target], lr=0.2)\n",
    "    optimizer = optim.Adam([target], lr=learning_rate)\n",
    "    \n",
    "    for ii in range(1, steps+1):\n",
    "\n",
    "        target_features = get_features(target, vgg)\n",
    "\n",
    "        # display intermediate images and print the loss\n",
    "        if  show_every is not None and ii % show_every == 0:\n",
    "            print(ii, report_loss(target_features, content_features, style_grams, \n",
    "                                  style_weights, content_weight, style_weight))\n",
    "            #plt.imshow(im_convert(target))\n",
    "            #plt.show()\n",
    "            #reporter(true_target_loss=true_target_loss(true_target, target).cpu().item())\n",
    "        \n",
    "        # closure() is required for LBFGS\n",
    "        # see example at https://pytorch.org/docs/stable/optim.html\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = style_transfer_loss(target_features, content_features, style_grams, \n",
    "                                             style_weights, content_weight, style_weight)\n",
    "            total_loss.backward(retain_graph=True)\n",
    "            \n",
    "            #reporter(total_loss=total_loss.cpu().item()) # report metrics\n",
    "            \n",
    "            return total_loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "    \n",
    "    #save new target image\n",
    "    target_name = \"_\".join(\n",
    "        [ k+\"_\"+str(config[k]).replace(', ','_').replace('[','').replace(']','') for k in config_keys ]\n",
    "    )\n",
    "    rgb = (im_convert(target)*255).astype('uint8')\n",
    "    Image.fromarray(rgb).save(\"target\"+target_name+\".png\")\n",
    "    \n",
    "    # this is for tune\n",
    "    reporter(true_target_loss=true_target_loss(true_target, target).cpu().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search with Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n",
      "WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-01-07_21-20-18_22662/logs.\n",
      "Waiting for redis server at 127.0.0.1:31567 to respond...\n",
      "Waiting for redis server at 127.0.0.1:40475 to respond...\n",
      "Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\n",
      "Starting the Plasma object store with 20.0 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8890/notebooks/ray_ui.ipynb?token=5dd2224099751223de4f5574b808b18d7345cdad994ce5d1\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.2.7',\n",
       " 'redis_address': '192.168.2.7:31567',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-01-07_21-20-18_22662/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-01-07_21-20-18_22662/sockets/raylet'],\n",
       " 'webui_url': 'http://localhost:8890/notebooks/ray_ui.ipynb?token=5dd2224099751223de4f5574b808b18d7345cdad994ce5d1'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install ray\n",
    "import ray\n",
    "import ray.tune as tune\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'van_gogh_20190107-2120'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# come up with a name for the folder where \"experiment\" results will be stored\n",
    "exp_name = \"van_gogh_\"+ datetime.datetime.now().strftime('%Y%m%d-%H%M')\n",
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "\n",
      "Created LogSyncer for /home/artem/ray_results/van_gogh_20190107-2120/generate_image_with_config_0_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]_2019-01-07_21-26-16nrs32v2j -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/6 CPUs, 1/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /home/artem/ray_results/van_gogh_20190107-2120\n",
      "PENDING trials:\n",
      " - generate_image_with_config_1_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_2_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_3_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_4_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_5_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_6_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_7_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_8_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_9_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_10_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_11_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_12_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      " - generate_image_with_config_13_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      " - generate_image_with_config_14_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      "RUNNING trials:\n",
      " - generate_image_with_config_0_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tRUNNING\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /home/artem/ray_results/van_gogh_20190107-2120\n",
      "PENDING trials:\n",
      " - generate_image_with_config_1_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_2_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_3_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_4_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_5_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_6_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_7_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_8_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_9_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_10_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_11_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_12_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      " - generate_image_with_config_13_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      " - generate_image_with_config_14_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      "TERMINATED trials:\n",
      " - generate_image_with_config_0_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tTERMINATED [pid=22694], 172 s, 1 iter\n",
      "\n",
      "Created LogSyncer for /home/artem/ray_results/van_gogh_20190107-2120/generate_image_with_config_1_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]_2019-01-07_21-29-09rijhj1wf -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /home/artem/ray_results/van_gogh_20190107-2120\n",
      "PENDING trials:\n",
      " - generate_image_with_config_2_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_3_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_4_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_5_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_6_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_7_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_8_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_9_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_10_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_11_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_12_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      " - generate_image_with_config_13_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      " - generate_image_with_config_14_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      "TERMINATED trials:\n",
      " - generate_image_with_config_0_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tTERMINATED [pid=22694], 172 s, 1 iter\n",
      " - generate_image_with_config_1_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tTERMINATED [pid=22698], 174 s, 1 iter\n",
      "\n",
      "Created LogSyncer for /home/artem/ray_results/van_gogh_20190107-2120/generate_image_with_config_2_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]_2019-01-07_21-32-03xs7zof1h -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /home/artem/ray_results/van_gogh_20190107-2120\n",
      "PENDING trials:\n",
      " - generate_image_with_config_3_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_4_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_5_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_6_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_7_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_8_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_9_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_10_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_11_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_12_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      " - generate_image_with_config_13_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      " - generate_image_with_config_14_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      "TERMINATED trials:\n",
      " - generate_image_with_config_0_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tTERMINATED [pid=22694], 172 s, 1 iter\n",
      " - generate_image_with_config_1_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tTERMINATED [pid=22698], 174 s, 1 iter\n",
      " - generate_image_with_config_2_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tTERMINATED [pid=22695], 177 s, 1 iter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/artem/ray_results/van_gogh_20190107-2120/generate_image_with_config_3_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]_2019-01-07_21-35-01nmecazff -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/1 GPUs\n",
      "Unknown memory usage. Please run `pip install psutil` (or ray[debug]) to resolve)\n",
      "Result logdir: /home/artem/ray_results/van_gogh_20190107-2120\n",
      "PENDING trials:\n",
      " - generate_image_with_config_4_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_5_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_6_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_7_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_8_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.2, 0.1, 0.3, 0.1, 0.3]:\tPENDING\n",
      " - generate_image_with_config_9_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_10_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_11_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.1, 0.2, 0.5, 0.1, 0.1]:\tPENDING\n",
      " - generate_image_with_config_12_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      " - generate_image_with_config_13_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      " - generate_image_with_config_14_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.1, 0.1, 0.1, 0.6, 0.1]:\tPENDING\n",
      "TERMINATED trials:\n",
      " - generate_image_with_config_0_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tTERMINATED [pid=22694], 172 s, 1 iter\n",
      " - generate_image_with_config_1_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tTERMINATED [pid=22698], 174 s, 1 iter\n",
      " - generate_image_with_config_2_content_weight=10.0,steps=5000,style_weight=1.0,style_weights=[0.6, 0.1, 0.1, 0.1, 0.1]:\tTERMINATED [pid=22695], 177 s, 1 iter\n",
      " - generate_image_with_config_3_content_weight=0.1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]:\tTERMINATED [pid=22699], 179 s, 1 iter\n",
      "\n",
      "Created LogSyncer for /home/artem/ray_results/van_gogh_20190107-2120/generate_image_with_config_4_content_weight=1,steps=5000,style_weight=1.0,style_weights=[0.3, 0.1, 0.2, 0.1, 0.3]_2019-01-07_21-38-01j7e127xg -> \n"
     ]
    }
   ],
   "source": [
    "all_trials = tune.run_experiments({\n",
    "    exp_name: {\n",
    "        \"run\": generate_image_with_config,\n",
    "        \"stop\": {\"true_target_loss\": 0.1},\n",
    "        \"config\": {\n",
    "            'style_weights': tune.grid_search(\n",
    "                list(map(lambda x: [y/10 for y in x], filter(lambda x: not 0 in x, combinations_fixed_sum(10, 5))))[::30]\n",
    "            ),\n",
    "            'content_weight': tune.grid_search([1e-1, 1, 10.0]), \n",
    "            'style_weight': tune.grid_search([1.0]),\n",
    "            'steps': tune.grid_search([5000]),\n",
    "        },\n",
    "        \"resources_per_trial\": {\n",
    "            \"gpu\": 1, 'cpu': 1,\n",
    "        },\n",
    "    }\n",
    "},\n",
    "verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you get this running, \n",
    "\n",
    "you can run `tersorboard` as follow:\n",
    "\n",
    "`tensorboard --logdir=~/ray_results/my_experiment`\n",
    "\n",
    "and aim your browser at:\n",
    "\n",
    "`http://localhost:6006/`\n",
    "\n",
    "We are expecially interested at true_target_loss graph, as it will show the values. We are looking at the lowest number here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all images\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "all_images = sorted(glob(str(Path.home())+\"/ray_results/\"+exp_name+\"/*/*png\"))\n",
    "\n",
    "#this shoud equal to the number of the trials\n",
    "len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 3 #can be number of cotent weights in the search grid here, for viewability\n",
    "n_rows = len(all_images) // 3 + 1\n",
    "f, axarr = plt.subplots( n_rows , n_cols,  figsize=(6*n_cols, 6*n_rows))\n",
    "#f, axarr = plt.subplots( n_cols, len(all_images) // 3 + 1)\n",
    "curr_row = 0\n",
    "\n",
    "for index, im_name in enumerate(all_images):\n",
    "    fb = open(im_name, \"rb\")\n",
    "    a = plt.imread(fb)\n",
    "\n",
    "    # find the column by taking the current index modulo 3\n",
    "    col = index % n_cols\n",
    "    \n",
    "    # plot on relevant subplot\n",
    "    axarr[curr_row, col].imshow(a)\n",
    "    axarr[curr_row, col].axis('off')\n",
    "    #make plot title from image name, \n",
    "    #e.g targetstyle_weights_0.6_0.1_0.1_0.1_0.1_content_weight_1_style_weight_1.0_steps_5000.png\n",
    "    title = re.sub('\\.\\w+$', '', os.path.basename(im_name))\n",
    "    title1 = re.findall(\"[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?\\d+)?\", title)\n",
    "    title = \"{0}-{1}-{2}-{3}-{4}\\nstyle/content={6}/{5}\\nsteps={7}\".format(*title1)\n",
    "    axarr[curr_row, col].set_title(title)\n",
    "    if col == 2:\n",
    "         # we have finished the current row, so increment row counter\n",
    "        curr_row += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "* tune config style weights via chain\n",
    "* combinations_fixed_sum: add option to exclude conbinations with 0.\n",
    "* add learning rate to grid_search\n",
    "* redirect output of tune.run_experiments cell, so it doesn't clutter the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
